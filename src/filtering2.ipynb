{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = \"..\\\\data\\\\NOMAL.xlsx\"\n",
    "raw_df = pd.read_excel(path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features with constant values\n",
    "noInfo_columns = raw_df.columns[raw_df.nunique() <= 1] # columns with no values or with only the same value\n",
    "\n",
    "filtered_df = raw_df.loc[:, raw_df.nunique() > 1] # this operation also removes the empty columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to remove special characters from a string\n",
    "def remove_special_characters(string):\n",
    "    return re.sub(r'\\W+', '', string)\n",
    "\n",
    "# Rename the features in filtered_df\n",
    "filtered_df.rename(columns=lambda x: remove_special_characters(x), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the categories for data\n",
    "\n",
    "patient_general = [\"Codice identificativo:\", 'Sesso', 'Nazionalità', 'Età', 'Domicilio','Altezza','Peso', 'Sub-Saharan Africa', 'Pregressa malaria', 'Profilassi']\n",
    "comorbidities = ['Comorbilità', 'Diabete', 'HIV', 'Cirrosi', 'IRC']\n",
    "patient_state_arrival = ['GCS', 'Seizures', 'Prostrazione', 'Shock', 'Bleeding', 'ARDS', 'Anemia', 'Creatinina', 'Glicemia', 'Acidosi', 'Bilirubina', 'Hyperparasitaemia', 'Numero criteri']\n",
    "diagnosis_type = ['RDT', 'Emoscopia', 'NAAT']\n",
    "diagnosis_result = ['Falciparum', 'Ovale', 'Parassitemia valore assoluto', 'Percentuale parassitemia']\n",
    "baseline_other = ['PA sistolica', 'PA diastolica', 'FC', 'FR', 'Temperatura', 'Ritardo terapeutico']\n",
    "QTc = ['QTc ingresso', 'QTc dopo ACT', 'QTC dopo ultima somministrazione artesunato *']\n",
    "T0 =  [\"T0. [GB (in cell/ul):]\", \"T0. [GR (in cell/ul):]\", \"T0. [Hb (in g/dl):]\", \"T0. [PLT (in cell/ul):]\", \"T0. [Glicemia (in mg/dl):]\", \"T0. [Azotemia (in mg/dl):]\", \"T0. [Creatinina (in mg/dl):]\", \"T0. [LDH (in U/L):]\", \"T0. [AST (in U/L):]\", \"T0. [ALT (in U/L):]\", \"T0. [Bilirubina tot (in mg/dl):]\", \"T0. [Bilirubina diretta (in mg/dl):]\", \"T0. [Sodio (in mEq/l):]\", \"T0. [Potassio (in mEq/l):]\", \"T0. [Ca (in mg/dl):]\", \"T0. [INR:]\", \"T0. [fibrinogeno (in mg/dl):]\", \"T0. [pH:]\", \"T0. [bicarbonati (in mmol/l):]\", \"T0. [Lattati # (in mmol/l):\"]\n",
    "T1 = [\"T1. [Goccia spessa e striscio periferico:]\", \"T1. [TC (temperatura corporea) in °C:]\"]\n",
    "T2 = [\"T2. [Goccia spessa e striscio periferico:]\", \"T2. [TC (temperatura corporea) in °C:]\"]\n",
    "T3 = [\"T3. [Goccia spessa e striscio periferico:]\", \"T3. [TC (temperatura corporea) in °C:]\"]\n",
    "T7 = [ \"T7. [Goccia spessa e striscio periferico:]\",\"T7. [TC (temperatura corporea) in °C:]\"]\n",
    "treatment = [\"Artesunato ev: Somministrazione [1][Data inizio e ora]\", \"Artemether/ Lumefantrina: Somministrazione [1][Data inizio e ora]\", \"Diidroartemisinina/Piperachina: Somministrazione [1][Data inizio e ora]\", \"ACT\", \"Artesunato + ACT\", \"Atovaquone/Proguanile\", \"Doxiciclina per os\", \"Clindamicina\", \"Antibiotici\", \"Chinino\", \"Primachina\", \"Durata Artesunato\"]\n",
    "outcome = [\"Durata ricovero\", \"Decesso.\", \"ICU\", \"Eventuali sequele:\", \"PADH, post-artesunate delayed haemoly1s\", \"Insorgenza PADH\", \"Permanenza in Terapia Intensiva (giorni):\", \"Trasferimento in Rianimazione (anche in altro centro)?\", \"Guarigione\"]\n",
    "PADH_info = [\"Insorgenza PADH\", \"Si prega di fornire tutti i parameri vitali. [GB (in cell/ul):]\", \"Si prega di fornire tutti i parameri vitali. [Hb (in g/dl):]\", \"Si prega di fornire tutti i parameri vitali. [PLT (in cell/ul):]\", \"Si prega di fornire tutti i parameri vitali. [reticoliti (in cell/ul):]\", \"Si prega di fornire tutti i parameri vitali. [LDH (in U/l)lcio:]\", \"Si prega di fornire tutti i parameri vitali. [AST (U/L):]\", \"Si prega di fornire tutti i parameri vitali. [ALT (U/L):]\", \"Si prega di fornire tutti i parameri vitali. [Bilirubina tot (mg/dl):]\", \"Si prega di fornire tutti i parameri vitali. [Bilirubina diretta (mg/dl):]\", \"Si prega di fornire tutti i parameri vitali. [aptoglobina (in mg/dl):]\", \"Si prega di fornire tutti i parameri vitali. [test di Coombs diretto:]\", \"Si prega di fornire tutti i parameri vitali. [test di coombs indiretto:]\", \"Vuole riportare ulteriori informazioni relative ai prelievi effettuati durante le visite intermedie fino alla risoluzione dell’emolisi?\", \"Nadir Hb\", \"Trasfusione:\", \"Unità trasfuse\"]\n",
    "follow_up = [\"Altri eventi avversi\", \"Diarrea e disidratazione\", \"Ipertransaminasemia tardiva\", \"Polmonite\", \"IVU nosocomiale\", \"Esofagite\", \"Dispepsia\", \"Tachiaritmia sopraventricolare\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing special caracters from the category description\n",
    "\n",
    "import re\n",
    "\n",
    "# Function to remove special characters from a string\n",
    "def remove_special_characters(string):\n",
    "    return re.sub(r'\\W+', '', string)\n",
    "\n",
    "# Update the list of features for each category\n",
    "patient_general = [remove_special_characters(feature) for feature in patient_general]\n",
    "comorbidities = [remove_special_characters(feature) for feature in comorbidities]\n",
    "patient_state_arrival = [remove_special_characters(feature) for feature in patient_state_arrival]\n",
    "diagnosis_type = [remove_special_characters(feature) for feature in diagnosis_type]\n",
    "diagnosis_result = [remove_special_characters(feature) for feature in diagnosis_result]\n",
    "baseline_other = [remove_special_characters(feature) for feature in baseline_other]\n",
    "QTc = [remove_special_characters(feature) for feature in QTc]\n",
    "T0 = [remove_special_characters(feature) for feature in T0]\n",
    "T1 = [remove_special_characters(feature) for feature in T1]\n",
    "T2 = [remove_special_characters(feature) for feature in T2]\n",
    "T3 = [remove_special_characters(feature) for feature in T3]\n",
    "T7 = [remove_special_characters(feature) for feature in T7]\n",
    "treatment = [remove_special_characters(feature) for feature in treatment]\n",
    "outcome = [remove_special_characters(feature) for feature in outcome]\n",
    "PADH_info = [remove_special_characters(feature) for feature in PADH_info]\n",
    "follow_up = [remove_special_characters(feature) for feature in follow_up]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define the categories\n",
    "categories = [patient_general, comorbidities, patient_state_arrival, diagnosis_type, diagnosis_result, baseline_other, QTc, T0, T1, T2, T3, T7, treatment, outcome, PADH_info, follow_up]\n",
    "\n",
    "# Check if features belong to a category (partial match)\n",
    "missing_features = []\n",
    "for column in filtered_df.columns:\n",
    "    matched = False\n",
    "    for category in categories:\n",
    "        for feature in category:\n",
    "            if re.search(re.escape(feature), column, re.IGNORECASE):\n",
    "                matched = True\n",
    "                break\n",
    "        if matched:\n",
    "            break\n",
    "    if not matched:\n",
    "        missing_features.append(column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if noInfo_columns are present in categories and remove them\n",
    "removed_features = []\n",
    "for column in noInfo_columns:\n",
    "    for i, category in enumerate(categories):\n",
    "        if column in category:\n",
    "            categories[i].remove(column)\n",
    "            removed_features.append(column)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balth\\AppData\\Local\\Temp/ipykernel_2124/2777475353.py:20: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if filtered_df[feature].dtype != np.object:  # Check if the feature is not a string\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "\n",
    "# Define the category lists\n",
    "categories = [patient_general, comorbidities, patient_state_arrival, diagnosis_type, diagnosis_result, baseline_other, QTc, T0, T1, T2, T3, T7, treatment, outcome, PADH_info, follow_up]\n",
    "category_names = ['Patient General', 'Comorbidities', 'Patient State Arrival', 'Diagnosis Type', 'Diagnosis Result', 'Baseline Other', 'QTc', 'T0', 'T1', 'T2', 'T3', 'T7', 'Treatment', 'Outcome', 'PADH Info', 'Follow-up']\n",
    "\n",
    "# Get the 'ICU' column from filtered_df\n",
    "target_column = filtered_df['ICU']\n",
    "\n",
    "# Iterate over each category\n",
    "for category, category_name in zip(categories, category_names):\n",
    "    # Filter the features based on the current category\n",
    "    features = [feature for feature in filtered_df.columns if feature in category]\n",
    "\n",
    "    # Calculate the correlation between each feature and ICU\n",
    "    correlations = []\n",
    "    for feature in features:\n",
    "        if filtered_df[feature].dtype != np.object:  # Check if the feature is not a string\n",
    "            correlation = filtered_df[feature].corr(target_column)\n",
    "            correlations.append((feature, correlation))\n",
    "\n",
    "    # Sort the correlation values in ascending order\n",
    "    correlations.sort(key=lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a new dataframe to store the results\n",
    "feature_info = pd.DataFrame(columns=['feature', 'type', 'mean', 'variance'])\n",
    "\n",
    "# Iterate over the columns in the filtered dataframe\n",
    "for column in filtered_df.columns:\n",
    "    col_data = filtered_df[column]\n",
    "    col_type = ''\n",
    "    col_mean = ''\n",
    "    col_var_entropy = ''\n",
    "    \n",
    "    # Check if the column has string values\n",
    "    if col_data.dtype == object:\n",
    "        col_type = 'string'\n",
    "    elif set(col_data.dropna().unique()) == {0, 1}:\n",
    "        col_type = 'categorical'\n",
    "        col_mean = col_data.mean()\n",
    "        # col_var_entropy = np.nans\n",
    "    elif col_data.dtype == np.int64 or all(pd.isnull(val) or val.is_integer() for val in col_data.dropna().unique()):\n",
    "        col_type = 'int'\n",
    "        col_mean = col_data.mean()\n",
    "        col_var_entropy = col_data.var()\n",
    "    elif col_data.dtype == np.float64 or any('.' in str(val) for val in col_data.dropna().unique()):\n",
    "        col_type = 'float'\n",
    "        col_mean = col_data.mean()\n",
    "        col_var_entropy = col_data.var()\n",
    "    else:\n",
    "        col_type = 'unknown'\n",
    "\n",
    "    # Add the results to the new dataframe\n",
    "    feature_info = feature_info.append({'feature': column, 'type': col_type, 'mean': col_mean, 'variance': col_var_entropy},\n",
    "                                       ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.impute import KNNImputer\n",
    "\n",
    "# # Define the data types for imputation\n",
    "# data_types = {\n",
    "#     'categorical': np.int64,  # Categorical data type\n",
    "#     'int': np.int64,          # Integer data type\n",
    "#     'float': np.float64       # Float data type\n",
    "# }\n",
    "\n",
    "# # Identify the missing values in filtered_df\n",
    "# missing_values = filtered_df.isnull().sum()\n",
    "\n",
    "# # Separate features based on their data types\n",
    "# categorical_features = feature_info[feature_info['type'] == 'categorical']['feature'].tolist()\n",
    "# int_features = feature_info[feature_info['type'] == 'int']['feature'].tolist()\n",
    "# float_features = feature_info[feature_info['type'] == 'float']['feature'].tolist()\n",
    "\n",
    "# # Impute missing values for each data type\n",
    "# for data_type, features in [('categorical', categorical_features), ('int', int_features), ('float', float_features)]:\n",
    "#     # Filter features based on data type\n",
    "#     features_to_impute = [feature for feature in features if feature in missing_values.index and missing_values[feature] > 0]\n",
    "\n",
    "#     if len(features_to_impute) > 0:\n",
    "#         # Prepare the imputation array\n",
    "#         impute_array = filtered_df[features_to_impute].values\n",
    "\n",
    "#         if data_type in ['categorical', 'int']:\n",
    "#             # Perform imputation for 'categorical' and 'int' features using median strategy\n",
    "#             imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "#             imputed_values = imputer.fit_transform(impute_array)\n",
    "#             filtered_df.loc[:, features_to_impute] = np.round(imputed_values).astype(data_types[data_type])\n",
    "#         elif data_type == 'float':\n",
    "#             # Perform imputation for 'float' features using mean strategy\n",
    "#             imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "#             imputed_values = imputer.fit_transform(impute_array)\n",
    "#             filtered_df.loc[:, features_to_impute] = imputed_values\n",
    "\n",
    "# # Verify if any missing values remain in the DataFrame\n",
    "# missing_values_after_imputation = filtered_df.isnull().sum()\n",
    "# missing_values_to_print = missing_values_after_imputation[missing_values_after_imputation != 0]\n",
    "# if not missing_values_to_print.empty:\n",
    "#     print(f\"There are still missing values in the DataFrame after imputation:\\n{missing_values_to_print}\")\n",
    "# else:\n",
    "#     print(\"All missing values have been imputed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# # Select int and float features to normalize\n",
    "# numeric_features = feature_info[feature_info['type'].isin(['int', 'float'])]['feature'].tolist()\n",
    "\n",
    "# # Normalize numeric features\n",
    "# scaler = StandardScaler()  # or scaler = MinMaxScaler() for min-max normalization\n",
    "# filtered_df[numeric_features] = scaler.fit_transform(filtered_df[numeric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From now on we will drop the \"Codice identificativo\" because it's string data with no info.\n",
    "# So we make an updated version of Patient_general for prediciton\n",
    "patient_general_noID = ['Sesso', 'Nazionalità', 'Età', 'Domicilio','Altezza','Peso', 'Sub-Saharan Africa', 'Pregressa malaria', 'Profilassi']\n",
    "\n",
    "# And we will only keep the features that do not introduce \"bias\" in our graphe\n",
    "predictive_categories = [patient_general_noID, comorbidities, patient_state_arrival, diagnosis_result, baseline_other, T0, treatment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store selected feature names\n",
    "selected_features = []\n",
    "\n",
    "# Iterate over predictive_categories\n",
    "for category in predictive_categories:\n",
    "    # Check if any column in filtered_df is present in the current category\n",
    "    selected_features.extend([feature for feature in filtered_df.columns if feature in category])\n",
    "\n",
    "# Create predictive_df DataFrame with selected features\n",
    "predictive_df = filtered_df[selected_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Export the DataFrame to a CSV file\n",
    "file_name = \"../data/unfilled.csv\"\n",
    "predictive_df.to_csv(file_name, index=False)  # Set index=False to exclude the index column in the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2124/2235509928.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = []\n",
    "\n",
    "# Iterate over selected features\n",
    "for feature in selected_features:\n",
    "    # Calculate correlation between the selected feature and \"ICU\"\n",
    "    correlation = filtered_df[feature].corr(filtered_df[\"ICU\"])\n",
    "    correlations.append((feature, correlation))\n",
    "\n",
    "# Sort the correlations in descending order based on absolute values\n",
    "correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Print the feature/correlation pairs\n",
    "for feature, correlation in correlations:\n",
    "    print(f\"Feature: {feature}, Correlation with ICU: {correlation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Saving the correlations dictionary to a file\n",
    "with open('../data/correlations.pkl', 'wb') as file:\n",
    "    pickle.dump(correlations, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv('../data/filtered_df.csv', index=False)\n",
    "predictive_df.to_csv('../data/predictive_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
